{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a7813480-ce22-4459-bd6b-055c7b086fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.9-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Downloading psycopg2-2.9.9-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.3/1.2 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.0/1.2 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.9\n"
     ]
    }
   ],
   "source": [
    "#!pip install requests\n",
    "#!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a25c2e1-d425-4214-8458-1bc1bd514344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, when,concat, lit,regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import concat_ws\n",
    "import json\n",
    "import psycopg2\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "### Define JDBC path\n",
    "jar_path = os.path.abspath(\"./postgresql-42.7.4.jar\")\n",
    "\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Create Spark session including JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('pySparkSetup') \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7a68f0a-7d2e-4778-b0b3-2c0821cb3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB Connection\n",
    "url_db = \"jdbc:postgresql://localhost:5432/postgres\"  # Cambia la URL si tu DB tiene otro puerto o host\n",
    "properties = {\n",
    "    \"user\": \"postgres\",         # Coloca tu usuario de PostgreSQL\n",
    "    \"password\": \"postgres\",  # Coloca tu contraseña de PostgreSQL\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c7ea7d-beae-460d-9a34-a972126afbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: You should change the values to the database connection psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4717e636-2ac9-42f1-886b-03709d8223e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_now_str 2024-09-06T09:44:48-0700\n",
      "last_days_str 2024-08-30T09:44:48-0700\n"
     ]
    }
   ],
   "source": [
    "#Check currently date\n",
    "timezone = pytz.timezone('US/Pacific')\n",
    "\n",
    "# datetime variables containing current date\n",
    "date_now = datetime.now(timezone)\n",
    "\n",
    "# Substract 7 days to fetch the last week\n",
    "last_days=(date_now - timedelta(days=7))\n",
    "\n",
    "date_now_str = date_now.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "last_days_str = last_days.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "print(\"date_now_str\", date_now_str)\n",
    "print(\"last_days_str\", last_days_str)\n",
    "\n",
    "#Now I can get the last 7 days in the api endpoint https://api.weather.gov/stations/station_id/observations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cc66acd-b1c7-47c9-9de6-30cc7b94b93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://api.weather.gov/stations/0128W/observations?start=2024-08-30T09:44:48-0700&end=2024-09-06T09:44:48-0700\n"
     ]
    }
   ],
   "source": [
    "# Variables declaration, if I need change the statition ID or another params to the request \n",
    "\n",
    "#Here I can change for other value, if the solution is called in AWS GLUE for example or called it in State machine\n",
    "#The station_id will be in the arguments\n",
    "\n",
    "station_id= \"0128W\" #\"0112W\" \n",
    "\n",
    "\n",
    "# Build ulr with the parameters\n",
    "url_ep = f\"https://api.weather.gov/stations/{station_id}/observations?start={last_days_str}&end={date_now_str}\"\n",
    "\n",
    "\n",
    "print(\"URL:\", url_ep)\n",
    "\n",
    "# Fetch de data\n",
    "response = requests.get(url_ep)\n",
    "\n",
    "#If response == 200 it is OK\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    #Save the JSON in temporal file, I don't know the size of data so this way keep less memory in usage\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".json\", mode='w') as temp_file:\n",
    "        json.dump(data['features'], temp_file)\n",
    "        temp_filename = temp_file.name\n",
    "    # Read the json file\n",
    "    df = spark.read.json(temp_filename)\n",
    "else:\n",
    "    print(\"Error\", response.status_code)\n",
    "    print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "313406bf-523e-4d45-bb9c-8db24eff896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an UDF for get the name from other link properties.station with that I can go to the name,timeZone,coordinates of station.\n",
    "def get_station_info(station_url):\n",
    "    try:\n",
    "        # Fetch data,\n",
    "        response = requests.get(station_url)\n",
    "        # if response equal to 200 it is OK\n",
    "        if response.status_code == 200:\n",
    "            station_data = response.json()\n",
    "            \n",
    "            #Get the station name\n",
    "            station_name = station_data.get('properties', {}).get('name', \"Unknown\")\n",
    "            \n",
    "            #Get the station time_zone\n",
    "            time_zone = station_data.get('properties', {}).get('timeZone', \"Unknown\")\n",
    "            \n",
    "            #Get the station coordinates\n",
    "            coordinates = station_data.get('geometry', {}).get('coordinates', [\"Unknown\", \"Unknown\"])\n",
    "            \n",
    "            return (station_name, time_zone, coordinates[0], coordinates[1])  # Nombre, zona horaria, latitud, longitud    \n",
    "        else:\n",
    "            return (\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\")  # Default values\n",
    "    except Exception as e:\n",
    "        return (\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\")  # Exception default values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb3e4ba4-b79a-40b9-8523-615697aea8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux=df #Copy the data frame to avoid loose the original information\n",
    "\n",
    "df_selected = df_aux.select(\n",
    "    col(\"id\"),\n",
    "    col(\"properties.timestamp\").alias(\"timestamp\"),\n",
    "    col(\"properties.temperature.value\").alias(\"temperature_value\"),\n",
    "    col(\"properties.temperature.unitCode\").alias(\"temperature_unit\"),\n",
    "    col(\"properties.windSpeed.value\").alias(\"wind_speed_value\"),\n",
    "    col(\"properties.windSpeed.unitCode\").alias(\"wind_speed_unit\"),\n",
    "    col(\"properties.relativeHumidity.value\").alias(\"humidity_value\"),\n",
    "    col(\"properties.relativeHumidity.unitCode\").alias(\"humidity_unit\"),\n",
    "    col(\"properties.station\").alias(\"station_url\")\n",
    ")\n",
    "\n",
    "get_station_info_udf = udf(get_station_info, ArrayType(StringType()))\n",
    "\n",
    "#Using the UDF\n",
    "df_selected = df_selected.withColumn(\"station_info\", get_station_info_udf(col(\"station_url\")))\n",
    "\n",
    "#Splitting each data column\n",
    "df_selected = df_selected.withColumn(\"station_name\", col(\"station_info\")[0])\n",
    "df_selected = df_selected.withColumn(\"time_zone\", col(\"station_info\")[1])\n",
    "df_selected = df_selected.withColumn(\"latitude\", col(\"station_info\")[2])\n",
    "df_selected = df_selected.withColumn(\"longitude\", col(\"station_info\")[3])\n",
    "df_selected = df_selected.withColumn(\"station_id\",lit(station_id))\n",
    "df_selected =  df_selected.withColumn(\"temperature_unit\",regexp_replace(col(\"temperature_unit\"), \"wmoUnit:\", \"\"))\n",
    "df_selected =  df_selected.withColumn(\"humidity_unit\",regexp_replace(col(\"humidity_unit\"), \"wmoUnit:\", \"\"))\n",
    "df_selected =  df_selected.withColumn(\"wind_speed_unit\",regexp_replace(col(\"wind_speed_unit\"), \"wmoUnit:\", \"\"))\n",
    "\n",
    "\n",
    "#Drop column 'station_info' \n",
    "df_selected = df_selected.drop(\"station_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff6bbcbf-b5bb-43b5-b4f6-149c8ec9aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "#Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Verificar si la tabla existe\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = 'public' \n",
    "        AND table_name = 'weather_information'\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "#Check if the table exists\n",
    "table_exists = cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "if not table_exists:\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE public.weather_information (\n",
    "            station_id varchar NOT NULL,\n",
    "            station_name varchar NULL,\n",
    "            station_timezone varchar NULL,\n",
    "            latitude float8 NULL,\n",
    "            longitude float8 NULL,\n",
    "            observation_timestamp varchar NULL,\n",
    "            temperature varchar NULL,\n",
    "            temperature_unit varchar NULL,\n",
    "            wind_speed varchar NULL,\n",
    "            wind_speed_unit varchar NULL,\n",
    "            humidity varchar NULL,\n",
    "            humidity_unit varchar NULL,\n",
    "            createt_at timestamp(0) NULL DEFAULT now(),\n",
    "            station_url varchar NULL,\n",
    "            id varchar NULL\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    print(\"Table created successfully.\")\n",
    "else:\n",
    "    print(\"Table exists.\")\n",
    "\n",
    "# Cerrar la conexión\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4a955b0-5967-4333-b85d-62ead224998f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1095.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:832)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:832)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1545)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1530)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.buildReader(JsonFileFormat.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:285)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:576)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\r\n\tat org.apache.spark.sql.execution.BaseLimitExec.inputRDDs(limit.scala:168)\r\n\tat org.apache.spark.sql.execution.BaseLimitExec.inputRDDs$(limit.scala:167)\r\n\tat org.apache.spark.sql.execution.LocalLimitExec.inputRDDs(limit.scala:208)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute$(EvalPythonExec.scala:87)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.doExecute(BatchEvalPythonExec.scala:34)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     df_filtered \u001b[38;5;241m=\u001b[39m df_sql\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m max_timestamp)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Mostrar el DataFrame filtrado\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m df_filtered\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#Write data frame in database\u001b[39;00m\n\u001b[0;32m     40\u001b[0m df_filtered\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(url\u001b[38;5;241m=\u001b[39murl_db, table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_information\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties\u001b[38;5;241m=\u001b[39mproperties)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1095.showString.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:832)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:832)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1545)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1530)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.buildReader(JsonFileFormat.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:138)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:129)\r\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:285)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:548)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:537)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:576)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\r\n\tat org.apache.spark.sql.execution.BaseLimitExec.inputRDDs(limit.scala:168)\r\n\tat org.apache.spark.sql.execution.BaseLimitExec.inputRDDs$(limit.scala:167)\r\n\tat org.apache.spark.sql.execution.LocalLimitExec.inputRDDs(limit.scala:208)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute(EvalPythonExec.scala:88)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.doExecute$(EvalPythonExec.scala:87)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.doExecute(BatchEvalPythonExec.scala:34)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\r\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:51)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:498)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n"
     ]
    }
   ],
   "source": [
    "#Mapping each column to SQL table\n",
    "df_sql = df_selected.select(\n",
    "    col(\"station_id\"),\n",
    "    col(\"station_name\"),\n",
    "    col(\"time_zone\").alias(\"station_timezone\"),\n",
    "    when(col(\"longitude\") == \"Unknown\", None).otherwise(col(\"longitude\").cast(\"float\")).alias(\"longitude\"), \n",
    "    when(col(\"latitude\") == \"Unknown\", None).otherwise(col(\"latitude\").cast(\"float\")).alias(\"latitude\"), \n",
    "    col(\"timestamp\").alias(\"observation_timestamp\"),\n",
    "    col(\"temperature_value\").alias(\"temperature\"),\n",
    "    col(\"temperature_unit\"),\n",
    "    col(\"wind_speed_value\").alias(\"wind_speed\"),\n",
    "    col(\"wind_speed_unit\"),\n",
    "    col(\"humidity_value\").alias(\"humidity\"),\n",
    "    col(\"humidity_unit\"),\n",
    "    col(\"station_url\"),\n",
    "    col(\"id\")\n",
    ")\n",
    "\n",
    "#Query to get max  observation_timestamp in all table to ask Which information will be Insert or Not\n",
    "query = \"(SELECT MAX(observation_timestamp) as max_timestamp FROM public.weather_information WHERE station_id = '\"+station_id+\"' ) AS temp\"\n",
    "\n",
    "\n",
    "#Read query\n",
    "df_max_timestamp = spark.read.jdbc(url=url_db, table=query, properties=properties)\n",
    "\n",
    "\n",
    "max_timestamp = df_max_timestamp.collect()[0][\"max_timestamp\"]\n",
    "\n",
    "#Filter the data to Insert only the rows wich timestamp greater than the max timestamp in the table\n",
    "#If max timestamp does not exist will insert all data frame\n",
    "if max_timestamp is None:\n",
    "    df_filtered=df_sql\n",
    "else:\n",
    "    df_filtered = df_sql.filter(col(\"observation_timestamp\") > max_timestamp)\n",
    "\n",
    "# Mostrar el DataFrame filtrado\n",
    "df_filtered.show()\n",
    "\n",
    "#Write data frame in database\n",
    "df_filtered.write.jdbc(url=url_db, table=\"weather_information\", mode=\"append\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023d02b-5dc4-443b-9d53-14ffccad0955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
